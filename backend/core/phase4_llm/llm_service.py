"""
Phase 4: LLM Reasoning & Question Generation
Use LLM to understand project and generate intelligent interview questions
"""
from typing import List, Dict, Any
from pydantic import BaseModel
from groq import Groq
from loguru import logger
from config import settings


class ProjectSummary(BaseModel):
    """Project summary generated by LLM"""
    title: str
    description: str
    purpose: str
    tech_stack_summary: str
    key_features: List[str]
    workflow_description: str
    complexity_level: str  # beginner, intermediate, advanced


class InterviewQuestion(BaseModel):
    """Interview question"""
    question_text: str
    question_type: str  # why, what, how, where
    difficulty: str  # easy, medium, hard
    context: str  # Related code/file
    expected_keywords: List[str]
    evaluation_criteria: List[str]


class LLMService:
    """LLM Service for project understanding and question generation"""
    
    def __init__(self):
        self.client = Groq(api_key=settings.GROQ_API_KEY)
        self.model = settings.MODEL_NAME
        self.temperature = settings.TEMPERATURE
    
    def generate_project_summary(
        self,
        readme_content: str,
        tech_stack: List[str],
        frameworks: List[str],
        main_modules: List[str],
        sample_code: str = ""
    ) -> ProjectSummary:
        """Generate comprehensive project summary"""
        
        prompt = f"""You are an expert technical interviewer analyzing a candidate's GitHub project.

## Repository Information:

### README:
{readme_content[:2000]}

### Tech Stack:
{', '.join(tech_stack)}

### Frameworks:
{', '.join(frameworks)}

### Main Modules:
{', '.join(main_modules)}

### Sample Code:
{sample_code[:1000]}

## Task:
Analyze this project and provide a comprehensive summary including:

1. **Project Title**: A clear, concise title
2. **Description**: What the project does (2-3 sentences)
3. **Purpose**: Why this project exists, what problem it solves
4. **Tech Stack Summary**: Brief overview of technologies used and why
5. **Key Features**: List 3-5 main features or capabilities
6. **Workflow**: How the application works (user flow or system architecture)
7. **Complexity Level**: Assess as beginner, intermediate, or advanced

Return the analysis in JSON format with these keys:
- title
- description
- purpose
- tech_stack_summary
- key_features (array)
- workflow_description
- complexity_level
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert technical project analyst. Return ONLY valid JSON with no markdown or extra text."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature
            )
            
            content = response.choices[0].message.content.strip()
            
            # Extract JSON if wrapped in markdown code blocks
            if content.startswith("```"):
                content = content.split("```json")[-1] if "```json" in content else content.split("```")[-1]
                content = content.split("```")[0].strip()
            
            import json
            summary_dict = json.loads(content)
            
            return ProjectSummary(**summary_dict)
            
        except Exception as e:
            logger.error(f"Error generating project summary: {e}")
            logger.debug(f"Response content: {response.choices[0].message.content if 'response' in locals() else 'N/A'}")
            # Return default summary
            return ProjectSummary(
                title="Project Analysis",
                description="A software project",
                purpose="Project purpose not determined",
                tech_stack_summary=f"Uses {', '.join(tech_stack[:3])}",
                key_features=["Feature analysis pending"],
                workflow_description="Workflow analysis pending",
                complexity_level="intermediate"
            )
    
    def generate_interview_questions(
        self,
        project_summary: ProjectSummary,
        role_type: str,
        code_context: List[Dict[str, Any]],
        required_skills: List[str]
    ) -> List[InterviewQuestion]:
        """Generate contextual interview questions"""
        
        # Prepare code context
        context_text = "\n\n".join([
            f"### {ctx['metadata'].get('file_path', 'Unknown')}:\n{ctx['content'][:500]}"
            for ctx in code_context[:5]
        ])
        
        prompt = f"""You are an expert technical interviewer for a {role_type} internship role.

## Project Summary:
**Title**: {project_summary.title}
**Description**: {project_summary.description}
**Purpose**: {project_summary.purpose}
**Tech Stack**: {project_summary.tech_stack_summary}
**Complexity**: {project_summary.complexity_level}

## Key Features:
{chr(10).join([f"- {feature}" for feature in project_summary.key_features])}

## Code Context:
{context_text}

## Required Skills for {role_type}:
{', '.join(required_skills)}

## Task:
Generate {settings.MAX_QUESTIONS} intelligent interview questions about THIS SPECIFIC PROJECT.

### Question Types to Include:
1. **WHY Questions** (2-3): Understanding reasoning and decisions
   - Why did you choose X technology?
   - Why is this approach used?

2. **WHAT Questions** (2-3): Understanding components and functionality
   - What does this module do?
   - What is the purpose of X feature?

3. **HOW Questions** (2-3): Understanding implementation
   - How does X feature work?
   - How did you handle Y challenge?

4. **WHERE Questions** (1-2): Understanding architecture
   - Where is X logic implemented?
   - Where would you add Y feature?

### Requirements:
- Questions MUST be specific to the candidate's code
- Mix difficulty: {settings.MIN_QUESTIONS//2} easy/medium, rest medium/hard
- Focus on {role_type} role skills
- Include context/code snippets where relevant
- Provide expected keywords for evaluation

Return as JSON array with objects containing:
- question_text
- question_type (why/what/how/where)
- difficulty (easy/medium/hard)
- context (relevant code/file reference)
- expected_keywords (array of 3-5 keywords)
- evaluation_criteria (array of 2-3 criteria)
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert technical interviewer. Return ONLY valid JSON with no markdown or extra text."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature
            )
            
            content = response.choices[0].message.content.strip()
            
            # Extract JSON if wrapped in markdown code blocks
            if content.startswith("```"):
                content = content.split("```json")[-1] if "```json" in content else content.split("```")[-1]
                content = content.split("```")[0].strip()
            
            import json
            data = json.loads(content)
            
            # Extract questions array (handle different response formats)
            # Check if data is already a list/array
            if isinstance(data, list):
                questions_data = data
            elif isinstance(data, dict):
                questions_data = data.get('questions', [])
                # Check if the entire response is an array
                if not questions_data and 'items' in data:
                    questions_data = data['items']
            else:
                questions_data = []
            
            questions = []
            for q_data in questions_data[:settings.MAX_QUESTIONS]:
                try:
                    questions.append(InterviewQuestion(**q_data))
                except Exception as e:
                    logger.warning(f"Skipping invalid question: {e}")
                    continue
            
            logger.success(f"Generated {len(questions)} interview questions")
            return questions
            
        except Exception as e:
            logger.error(f"Error generating questions: {e}")
            # Return fallback questions
            return self._generate_fallback_questions(project_summary, role_type)
    
    def _generate_fallback_questions(self, project_summary: ProjectSummary, role_type: str) -> List[InterviewQuestion]:
        """Generate fallback questions if LLM fails"""
        return [
            InterviewQuestion(
                question_text=f"What is the main purpose of your {project_summary.title} project?",
                question_type="what",
                difficulty="easy",
                context="Project README",
                expected_keywords=["purpose", "goal", "solve", "user"],
                evaluation_criteria=["Clear explanation", "Understanding of project goal"]
            ),
            InterviewQuestion(
                question_text=f"Why did you choose {project_summary.tech_stack_summary.split()[1] if len(project_summary.tech_stack_summary.split()) > 1 else 'this technology'} for this project?",
                question_type="why",
                difficulty="medium",
                context="Technology choice",
                expected_keywords=["performance", "features", "suitable", "familiar"],
                evaluation_criteria=["Technical reasoning", "Decision making"]
            ),
            InterviewQuestion(
                question_text="How does the main workflow of your application function?",
                question_type="how",
                difficulty="medium",
                context="Application workflow",
                expected_keywords=["user", "process", "flow", "steps"],
                evaluation_criteria=["System understanding", "Architecture knowledge"]
            ),
            InterviewQuestion(
                question_text=f"What are the key features you implemented in this project?",
                question_type="what",
                difficulty="easy",
                context="Feature implementation",
                expected_keywords=["feature", "functionality", "capability"],
                evaluation_criteria=["Feature knowledge", "Implementation understanding"]
            ),
            InterviewQuestion(
                question_text="How would you improve or scale this project if given more time?",
                question_type="how",
                difficulty="hard",
                context="Future improvements",
                expected_keywords=["scalability", "optimization", "improvement", "feature"],
                evaluation_criteria=["Critical thinking", "Forward planning"]
            ),
            InterviewQuestion(
                question_text="Where is the main business logic implemented in your codebase?",
                question_type="where",
                difficulty="medium",
                context="Code architecture",
                expected_keywords=["module", "file", "folder", "component"],
                evaluation_criteria=["Code organization understanding", "Architecture awareness"]
            )
        ]


# Service instance
llm_service = LLMService()
